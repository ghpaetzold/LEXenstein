\chapter{The Substitution Ranking Module}
\label{rankers}

Substitution Ranking (SR) is the task of ranking a set of selected substitutions for a target complex word with respect to its simplicity. Approaches vary from simple word length and frequency-based measures \cite{Devlin1998,Carroll98,Carroll99,Biran2011} to more sophisticated linear combination as scoring functions \cite{uowshef} and Machine Learning approaches \cite{Horn2014}.

LEXenstein's SR module (lexenstein.rankers) provides access to $6$ approaches, each represented by a Python class. The following Sections describe each one individually.












\section{MetricRanker}

Employs a simple ranking strategy based on the values of a single feature provided by the user. By configuring the input FeatureEstimator object, the user can calculate values of several features for the candidates in a given dataset, and easily rank the candidates according to each of these features.

\subsection{Parameters}

During instantiation, the MetricRanker requires only a configured FeatureEstimator object, which must contain at least one feature that can be used as a metric for ranking. Once created, the user can retrieve rankings by using MetricRanker's ``getRankings'' function, which requires a VICTOR corpus and the index of a feature to be used as a ranking metric.

\subsection{Example}

The code snippet below shows the WSDSelector class being used:

\begin{lstlisting}
from lexenstein.features import *
from lexenstein.rankers import *

fe = FeatureEstimator()
fe.addCollocationalFeature('lm.txt', 0, 0, 'Complexity')
fe.addLengthFeature('Complexity')
fe.addSenseCountFeature('Simplicity')

mr = MetricRanker(fe)
frequency_ranks = mr.getRankings('lexmturk.txt', 0)
length_ranks = mr.getRankings('lexmturk.txt', 1)
sense_ranks = mr.getRankings('lexmturk.txt', 2)
\end{lstlisting}










\section{SVMRanker}

Use Support Vector Machines in a setup that minimizes a loss function with respect to a ranking model. In LS, this strategy is the one employed in the experiments of \cite{Horn2014}, yielding promising results.

\subsection{Parameters}

During instantiation, the SVMRanker requires for a FeatureEstimator object and a path to the root installation folder of SVM-Rank \cite{svmrank}. The user can then use the ``getFeaturesFile'', ``getTrainingModel'', ``getScoresFile'' and ``getRankings'' to train SVM ranking models and rank candidate substitutions. For more information on these functions' parameters, please refer to LEXenstein's documentation and the example in the following Section.

\subsection{Example}

The code snippet below shows the SVMRanker class being used:

\begin{lstlisting}
from lexenstein.features import *
from lexenstein.rankers import *

fe = FeatureEstimator()
fe.addCollocationalFeature('lm.txt', 0, 0, 'Complexity')
fe.addLengthFeature('Complexity')
fe.addSenseCountFeature('Simplicity')

svmr = SVMRanker(fe, '/svm-rank/')
svmr.getFeaturesFile('lexmturk.txt', 'features.txt')
svmr.getTrainingModel('features.txt', 0.1, 0.1, 0, 'model.txt')
svmr.getScoresFile('features.txt', 'model.txt', 'scores.txt')
rankings = svmr.getRankings('features.txt', 'scores.txt')
\end{lstlisting}



















\section{BoundaryRanker}

Employs a novel strategy, in which ranking is framed as a binary classification task. During training, this approach assigns the label $1$ to all candidates of rank $1\geq r \geq p$, where $p$ is a range set by the user, and $0$ to the remaining candidates. It then trains a stochastic descent linear classifier based on the features specified in the FeatureEstimator object. During testing, candidate substitutions are ranked based on how far from $0$ they are.

\subsection{Parameters}

During instantiation, the BoundaryRanker requires only for a FeatureEstimator object. The user can then use the ``trainRanker'' function to train a ranking model, and the ``getRankings'' to rank the candidates of a VICTOR corpus. For more information on the training parameters supported by the ``trainRanker'' function, please refer to LEXenstein's documentation.

\subsection{Example}

The code snippet below shows the BoundaryRanker class being used:

\begin{lstlisting}
from lexenstein.features import *
from lexenstein.rankers import *

fe = FeatureEstimator()
fe.addCollocationalFeature('lm.txt', 0, 0, 'Complexity')
fe.addLengthFeature('Complexity')
fe.addSenseCountFeature('Simplicity')

br = BoundaryRanker(fe)
br.trainRanker('lexmturk.txt', 1, 'modified_huber', 'l1', 0.1, 0.1, 0.001)
rankings = br.getRankings('lexmturk.txt')
\end{lstlisting}















\section{SVMBoundaryRanker}

Employs the same strategy used by the BoundaryRanker class, but instead of learning a linear ranking model through Stochastic Gradient Descent, it learns a linear or non-linear model by using Support Vector Machines.

\subsection{Parameters}

During instantiation, the SVMBoundaryRanker requires only for a FeatureEstimator object. The user can then use the ``trainRanker'' function to train a ranking model, and the ``getRankings'' to rank the candidates of a VICTOR corpus. For more information on the training parameters supported by the ``trainRanker'' function, please refer to LEXenstein's documentation.

\subsection{Example}

The code snippet below shows the SVMBoundaryRanker class being used:

\begin{lstlisting}
from lexenstein.features import *
from lexenstein.rankers import *

fe = FeatureEstimator()
fe.addCollocationalFeature('lm.txt', 0, 0, 'Complexity')
fe.addLengthFeature('Complexity')
fe.addSenseCountFeature('Simplicity')

sbr = SVMBoundaryRanker(fe)
sbr.trainRanker('lexmturk.txt', 1, 10, 'poly', 2, 0.1, 1)
rankings = sbr.getRankings('lexmturk.txt')
\end{lstlisting}











\section{BiranRanker}

Employs the strategy of \cite{Biran2011}, which models complexity as a function of a word's length and frequency in corpora of complex and simple content. As input, it requires for a language model trained over complex data, and a language model trained over simple data.

\subsection{Parameters}

The language models required by the BiranRanker must be binary, and must be produced by KenLM with the following command lines:

\begin{lstlisting}
lmplz -o [order] <[corpus_of_text] >[language_model_name]
\end{lstlisting}
\begin{lstlisting}
build_binary [language_model_name] [binary_language_model_name]
\end{lstlisting}

Complex and simple data can be downloaded fro David Kauchak's page\footnote{http://www.cs.pomona.edu/~dkauchak/simplification/}, or extracted from other sources.

\subsection{Example}

The code snippet below shows the BiranRanker class being used:

\begin{lstlisting}
from lexenstein.rankers import *

br = BiranRanker('lmc.bin', 'lms.bin')
rankings = br.getRankings('lexmturk.txt')
\end{lstlisting}















\section{YamamotoRanker}

Employs the strategy of \cite{Yamamoto2013}, which ranks words according to a weighted function that considers their frequency in a corpus of simple content, co-occurrence frequency with a target complex word, sense distance, point-wise mutual information and trigram frequencies. As input, it requires for a language model trained over a corpus of simple data and a co-occurrence model.

\subsection{Parameters}

The language model required by the YamamotoRanker must be binary, and must be produced by KenLM with the following command lines:

\begin{lstlisting}
lmplz -o [order] <[corpus_of_text] >[language_model_name]
\end{lstlisting}
\begin{lstlisting}
build_binary [language_model_name] [binary_language_model_name]
\end{lstlisting}

Complex and simple data can be downloaded fro David Kauchak's page\footnote{http://www.cs.pomona.edu/~dkauchak/simplification/}, or extracted from other sources. The co-occurrence model must be in plain text format, and each line must follow the format illustrated in Example~\ref{cooc}, where $\left\langle w_{i} \right\rangle$ is a word, $\left\langle c_{i}^{j} \right\rangle$ a co-occurring word and $\left\langle f_{i}^{j} \right\rangle$ its frequency of appearance.

\begin{equation}
\label{cooc}
\left\langle w_{i} \right\rangle\; \left\langle c_{i}^{0} \right\rangle\!:\!\left\langle f_{i}^{0} \right\rangle\;\left\langle c_{i}^{1} \right\rangle\!:\!\left\langle f_{i}^{1} \right\rangle\cdots\left\langle c_{i}^{n-1} \right\rangle\!:\!\left\langle f_{i}^{n-1} \right\rangle \; \left\langle c_{i}^{n} \right\rangle\!:\!\left\langle f_{i}^{n} \right\rangle
\end{equation}

Each component in the format above must be separated by a tabulation marker. To create a co-occurrence model, either create a script that does so, or follow the steps below:

\begin{enumerate}
\item Gather a corpus of text composed of one tokenized and truecased sentence per line.
\item Run the script \url{resources/scripts/Produce_Co-occurrence_Model.py} with the following command line:

\begin{lstlisting}
python Produce_Co-occurrence_Model.py <corpus> <window> <model_path>
\end{lstlisting}

Where ``<window>'' is the number of tokens to the left and right of a word to be included as a co-occurring word.

\end{enumerate}

To produce models faster, you can split your corpus in various small portions, run parallel processes to produce various small models, and then join them. For more information on the parameters of each function of the YamamotoRanker class, please refer to the LEXenstein documentation.

\subsection{Example}

The code snippet below shows the YamamotoRanker class being used:

\begin{lstlisting}
from lexenstein.rankers import *

yr = YamamotoRanker('lms.bin', 'cooc_model.txt')
rankings = yr.getRankings('lexmturk.txt')
\end{lstlisting}















\section{BottRanker}

Employs the strategy of \cite{Bott2012}, which ranks words according to a weighted function that considers their length and frequency in a corpus of simple content. As input, it requires for a language model trained over a corpus of simple data.

\subsection{Parameters}

The language model required by the BottRanker must be binary, and must be produced by KenLM with the following command lines:

\begin{lstlisting}
lmplz -o [order] <[corpus_of_text] >[language_model_name]
\end{lstlisting}
\begin{lstlisting}
build_binary [language_model_name] [binary_language_model_name]
\end{lstlisting}

Complex and simple data can be downloaded fro David Kauchak's page\footnote{http://www.cs.pomona.edu/~dkauchak/simplification/}, or extracted from other sources.

\subsection{Example}

The code snippet below shows the BottRanker class being used:

\begin{lstlisting}
from lexenstein.rankers import *

br = BottRanker('lms.bin')
rankings = br.getRankings('lexmturk.txt')
\end{lstlisting}











\section{GlavasRanker}

Employs the strategy of \cite{glavas2015}, which ranks words according to their average ranking, as determined by a given set of features. As input, it requires for a configured FeatureEstimator object.

\subsection{Parameters}

The FeatureEstimator object required must contain each and every feature that the user wishes to rank candidates with.

\subsection{Example}

The code snippet below shows the GlavasRanker class being used:

\begin{lstlisting}
from lexenstein.features import *
from lexenstein.rankers import *

fe = FeatureEstimator()
fe.addCollocationalFeature('lm.txt', 0, 0, 'Complexity')
fe.addLengthFeature('Complexity')
fe.addSenseCountFeature('Simplicity')

gr = GlavasRanker(fe)
rankings = gr.getRankings('lexmturk.txt')
\end{lstlisting}